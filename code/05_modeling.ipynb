{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "\n",
    "# nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "\n",
    "# Silence Future Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corpora from spaCy and gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stop_words(word_list, list_stop_words):\n",
    "    for word in word_list:\n",
    "        list_stop_words.add(word)\n",
    "        \n",
    "def remove_stop_words(word_list, list_stop_words):\n",
    "    for word in word_list:\n",
    "        list_stop_words.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Stop Words from NLTK and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_spacy = set(nlp.Defaults.stop_words)\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "full_stop_words = stop_words_spacy.union(stop_words_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify Stop Words Based on EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_add = ['like', 'know', 'want', 'feel', 'going', 'think', 'reddit', 'imgur', 'pron', 'officially', 'story', 'month', 'week', 'time', 'day', 'year']\n",
    "\n",
    "add_stop_words(words_to_add, full_stop_words)\n",
    "remove_stop_words(['not'], full_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "health = pd.read_csv('../data/womens_health_preprocessed.csv', lineterminator='\\n')\n",
    "obsgyn = pd.read_csv('../data/fertility_and_pregnancy_preprocessed.csv')\n",
    "pospar = pd.read_csv('../data/postpartum_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: LDA with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Digits from Text\n",
    "After initial LDA models, it became clear that, while some numbers may occur frequently in posts, they do not add a lot of value to deriving meaning from the topics. Therefore, digits will be removed from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(df, column1, column2):\n",
    "    df[column1] = df[column1].str.replace('\\d+', '', regex = True)\n",
    "    df[column2] = df[column2].str.replace('\\d+', '', regex = True)\n",
    "    \n",
    "# https://stackoverflow.com/questions/47010044/how-to-remove-numeric-characters-present-in-countvectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Build LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lda(df, column, n_topics, stop_words = full_stop_words):\n",
    "    tvec = TfidfVectorizer(max_df = 0.98, min_df = 0.005, ngram_range = (1,2), stop_words = stop_words)\n",
    "    dtm = tvec.fit_transform(df[column])\n",
    "    \n",
    "    LDA = LatentDirichletAllocation(n_components = n_topics, random_state = 42)\n",
    "    LDA.fit(dtm)\n",
    "    \n",
    "    vocab = tvec.get_feature_names()\n",
    "    topics = LDA.components_\n",
    "    \n",
    "    print(f'The number of topics is {n_topics}.')\n",
    "    print(f'Log-Likelihood Score: {round(LDA.score(dtm), 3)}')\n",
    "    print(f'Perplexity Score: {round(LDA.perplexity(dtm), 3)}')\n",
    "    print('\\n')\n",
    "    \n",
    "    for index, topic in enumerate(topics):\n",
    "        print(f'The top 15 words for topic {index}')\n",
    "        print([vocab[index] for index in topic.argsort()[-15:]])\n",
    "        print('\\n')\n",
    "    \n",
    "    return LDA.perplexity(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_sk_lda(df, column, n_topics, step_size, stop_words = full_stop_words):\n",
    "    for i in range(0, n_topics+1, step_size):\n",
    "        build_lda(df, column, n_topics, stop_words = full_stop_words)\n",
    "        print('- - - - - - - - - - - - - - - - - -')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Num Topics for General Health Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of topics is 17.\n",
      "Log-Likelihood Score: -1142793.108\n",
      "Perplexity Score: 2347.033\n",
      "\n",
      "\n",
      "The top 15 words for topic 0\n",
      "['party', 'instagram', 'man', 'bar', 'view', 'friend', 'girl', 'ex', 'deal', 'gift', 'app', 'favorite', 'boyfriend', 'guy', 'date']\n",
      "\n",
      "\n",
      "The top 15 words for topic 1\n",
      "['pelvic', 'fibroid', 'left', 'uterus', 'remove', 'doctor', 'cm', 'surgery', 'ovarian cyst', 'pain', 'ultrasound', 'ovary', 'ovarian', 'delete', 'cyst']\n",
      "\n",
      "\n",
      "The top 15 words for topic 2\n",
      "['small', 'notice', 'pain', 'finger', 'bump', 'nipple', 'cup', 'lump', 'labia', 'sex', 'hurt', 'tampon', 'not', 'vagina', 'breast']\n",
      "\n",
      "\n",
      "The top 15 words for topic 3\n",
      "['college', 'book', 'not', 'help', 'love', 'live', 'need', 'good', 'home', 'advice', 'new', 'interview', 'tip', 'work', 'job']\n",
      "\n",
      "\n",
      "The top 15 words for topic 4\n",
      "['weight gain', 'not', 'gain weight', 'self', 'fat', 'food', 'exercise', 'healthy', 'lose weight', 'diet', 'lose', 'body', 'gain', 'eat', 'weight']\n",
      "\n",
      "\n",
      "The top 15 words for topic 5\n",
      "['hormone', 'control pill', 'bc', 'not', 'bleed', 'heavy', 'normal', 'cycle', 'start', 'stop', 'pill', 'birth', 'control', 'birth control', 'period']\n",
      "\n",
      "\n",
      "The top 15 words for topic 6\n",
      "['shirt', 'fit', 'size', 'find', 'style', 'jean', 'not', 'good', 'makeup', 'clothe', 'buy', 'bra', 'look', 'dress', 'wear']\n",
      "\n",
      "\n",
      "The top 15 words for topic 7\n",
      "['pubic', 'not', 'ingrown hair', 'help', 'try', 'bikini', 'ingrown', 'area', 'use', 'leg', 'razor', 'wax', 'skin', 'hair', 'shave']\n",
      "\n",
      "\n",
      "The top 15 words for topic 8\n",
      "['experience', 'advice', 'child', 'mental health', 'insurance', 'need', 'life', 'depression', 'mental', 'anxiety', 'not', 'abortion', 'help', 'woman', 'health']\n",
      "\n",
      "\n",
      "The top 15 words for topic 9\n",
      "['birth control', 'birth', 'normal', 'bleeding', 'not', 'light', 'discharge', 'start', 'spot', 'sex', 'brown', 'blood', 'pill', 'bleed', 'period']\n",
      "\n",
      "\n",
      "The top 15 words for topic 10\n",
      "['test', 'doctor', 'vagina', 'symptom', 'pee', 'not', 'sex', 'antibiotic', 'bv', 'smell', 'uti', 'discharge', 'yeast infection', 'yeast', 'infection']\n",
      "\n",
      "\n",
      "The top 15 words for topic 11\n",
      "['thick', 'thin', 'face', 'color', 'dry', 'long', 'product', 'cut', 'grow', 'shampoo', 'dye', 'wash', 'look', 'nail', 'hair']\n",
      "\n",
      "\n",
      "The top 15 words for topic 12\n",
      "['try', 'life', 'ask', 'good', 'date', 'work', 'girl', 'tell', 'relationship', 'thing', 'talk', 'guy', 'people', 'not', 'friend']\n",
      "\n",
      "\n",
      "The top 15 words for topic 13\n",
      "['shot', 'mirena iud', 'insert', 'depo', 'experience', 'string', 'implant', 'kyleena', 'remove', 'insertion', 'nexplanon', 'copper iud', 'copper', 'mirena', 'iud']\n",
      "\n",
      "\n",
      "The top 15 words for topic 14\n",
      "['not', 'abnormal', 'cancer', 'obgyn', 'tell', 'colposcopy', 'result', 'test', 'exam', 'hpv', 'appointment', 'doctor', 'pap smear', 'smear', 'pap']\n",
      "\n",
      "\n",
      "The top 15 words for topic 15\n",
      "['ovulation', 'come', 'period late', 'symptom', 'th', 'sex', 'not', 'cycle', 'negative', 'pregnancy test', 'late', 'test', 'pregnant', 'pregnancy', 'period']\n",
      "\n",
      "\n",
      "The top 15 words for topic 16\n",
      "['symptom', 'painful', 'hurt', 'normal', 'doctor', 'experience', 'sleep', 'start', 'sex', 'low', 'not', 'bad', 'cramp', 'period', 'pain']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "health_sk_lda = build_lda(health, 'lemma_tokens', 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Num Topics for Fertility and Pregnancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of topics is 15.\n",
      "Log-Likelihood Score: -3399668.159\n",
      "Perplexity Score: 1990.693\n",
      "\n",
      "\n",
      "The top 15 words for topic 0\n",
      "['need', 'pay', 'pregnancy', 'insurance', 'covid', 'home', 'plan', 'hospital', 'pregnant', 'leave', 'baby', 'birth', 'job', 'not', 'work']\n",
      "\n",
      "\n",
      "The top 15 words for topic 1\n",
      "['faint', 'early', 'not', 'line', 'symptom', 'sex', 'late', 'delete', 'pregnancy test', 'negative', 'positive', 'pregnancy', 'pregnant', 'period', 'test']\n",
      "\n",
      "\n",
      "The top 15 words for topic 2\n",
      "['experience', 'normal', 'heartbeat', 'tell', 'result', 'today', 'pregnancy', 'not', 'appointment', 'baby', 'test', 'doctor', 'scan', 'blood', 'ultrasound']\n",
      "\n",
      "\n",
      "The top 15 words for topic 3\n",
      "['hungry', 'craving', 'lose', 'baby', 'weight gain', 'lbs', 'pound', 'healthy', 'pregnant', 'not', 'pregnancy', 'food', 'gain', 'weight', 'eat']\n",
      "\n",
      "\n",
      "The top 15 words for topic 4\n",
      "['feed', 'clothing', 'newborn', 'pack', 'formula', 'cloth', 'bottle', 'hospital', 'milk', 'breastfeed', 'breast', 'bag', 'diaper', 'pump', 'baby']\n",
      "\n",
      "\n",
      "The top 15 words for topic 5\n",
      "['not', 'birth', 'af', 'cramp', 'bleed', 'control', 'spot', 'start', 'birth control', 'implantation', 'pill', 'dpo', 'cycle', 'ttc', 'period']\n",
      "\n",
      "\n",
      "The top 15 words for topic 6\n",
      "['love', 'find', 'kid', 'thing', 'child', 'pregnancy', 'mom', 'people', 'husband', 'friend', 'family', 'pregnant', 'tell', 'not', 'baby']\n",
      "\n",
      "\n",
      "The top 15 words for topic 7\n",
      "['buy', 'look', 'fit', 'gender', 'boy', 'boob', 'girl', 'pant', 'nipple', 'dress', 'size', 'clothe', 'bra', 'wear', 'maternity']\n",
      "\n",
      "\n",
      "The top 15 words for topic 8\n",
      "['push', 'come', 'not', 'cm', 'pain', 'hour', 'hospital', 'induction', 'epidural', 'birth', 'induce', 'section', 'baby', 'contraction', 'labor']\n",
      "\n",
      "\n",
      "The top 15 words for topic 9\n",
      "['baby', 'thing', 'house', 'food', 'home', 'today', 'pregnant', 'dog', 'smell', 'husband', 'work', 'eat', 'not', 'tic', 'cry']\n",
      "\n",
      "\n",
      "The top 15 words for topic 10\n",
      "['car', 'gift', 'item', 'baby shower', 'good', 'look', 'recommendation', 'crib', 'stroller', 'seat', 'shower', 'nursery', 'registry', 'buy', 'baby']\n",
      "\n",
      "\n",
      "The top 15 words for topic 11\n",
      "['cramp', 'stomach', 'experience', 'nausea', 'normal', 'pregnant', 'hurt', 'wake', 'help', 'pregnancy', 'night', 'bad', 'not', 'sleep', 'pain']\n",
      "\n",
      "\n",
      "The top 15 words for topic 12\n",
      "['not', 'track', 'positive opk', 'start', 'lh', 'app', 'temp', 'positive', 'period', 'test', 'ovulate', 'cd', 'opk', 'ovulation', 'cycle']\n",
      "\n",
      "\n",
      "The top 15 words for topic 13\n",
      "['test', 'ivf', 'pregnancy', 'iui', 'husband', 'sperm', 'doctor', 'start', 'cycle', 'conceive', 'not', 'ttc', 'pregnant', 'fertility', 'try']\n",
      "\n",
      "\n",
      "The top 15 words for topic 14\n",
      "['braxton', 'little', 'not', 'pregnant', 'trimester', 'bump', 'pregnancy', 'dream', 'movement', 'morning', 'morning sickness', 'sickness', 'belly', 'baby', 'kick']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=15, random_state=42)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_lda(obsgyn, 'lemma_tokens', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Num Topics for Pospartum Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of topics is 30.\n",
      "Log-Likelihood Score: -1994441.195\n",
      "Perplexity Score: 3170.134\n",
      "\n",
      "\n",
      "The top 15 words for topic 0\n",
      "['conceive', 'late', 'blood', 'glucose', 'not', 'early', 'negative', 'cycle', 'symptom', 'pregnancy test', 'pregnant', 'positive', 'pregnancy', 'period', 'test']\n",
      "\n",
      "\n",
      "The top 15 words for topic 1\n",
      "['pee', 'labor', 'experience', 'blood pressure', 'discharge', 'normal', 'not', 'blood', 'braxton hicks', 'hicks', 'pressure', 'contraction', 'braxton', 'pain', 'cramp']\n",
      "\n",
      "\n",
      "The top 15 words for topic 2\n",
      "['mom', 'advance', 'help', 'find', 'read', 'recommendation', 'baby', 'online', 'doula', 'experience', 'thank', 'birth', 'book', 'breastfeed', 'class']\n",
      "\n",
      "\n",
      "The top 15 words for topic 3\n",
      "['risk', 'sick', 'vitamin', 'medication', 'covid', 'infection', 'shot', 'cold', 'cough', 'safe', 'not', 'flu', 'doctor', 'pregnant', 'pregnancy']\n",
      "\n",
      "\n",
      "The top 15 words for topic 4\n",
      "['need', 'talk', 'try', 'mom', 'thing', 'tell', 'pregnancy', 'husband', 'love', 'life', 'kid', 'child', 'pregnant', 'not', 'baby']\n",
      "\n",
      "\n",
      "The top 15 words for topic 5\n",
      "['not', 'cm', 'check', 'start', 'water', 'come', 'baby', 'pain', 'hour', 'birth', 'push', 'hospital', 'labor', 'epidural', 'contraction']\n",
      "\n",
      "\n",
      "The top 15 words for topic 6\n",
      "['not', 'look', 'amazon', 'thing', 'bag', 'maternity', 'need', 'nursery', 'item', 'registry', 'crib', 'diaper', 'clothe', 'buy', 'baby']\n",
      "\n",
      "\n",
      "The top 15 words for topic 7\n",
      "['dairy', 'pound', 'healthy', 'meal', 'not', 'weight gain', 'lose', 'craving', 'diet', 'pregnancy', 'food', 'delete', 'gain', 'weight', 'eat']\n",
      "\n",
      "\n",
      "The top 15 words for topic 8\n",
      "['efface', 'tomorrow', 'cm dilate', 'plug', 'membrane', 'baby', 'today', 'date', 'sweep', 'cervix', 'dilate', 'cm', 'induction', 'induce', 'labor']\n",
      "\n",
      "\n",
      "The top 15 words for topic 9\n",
      "['pregnant', 'throw', 'sick', 'bad', 'movement', 'eat', 'stomach', 'not', 'kick', 'pregnancy', 'trimester', 'morning', 'morning sickness', 'sickness', 'nausea']\n",
      "\n",
      "\n",
      "The top 15 words for topic 10\n",
      "['bed', 'bathroom', 'house', 'nursery', 'sit', 'clean', 'hair', 'dog', 'baby', 'smell', 'husband', 'cry', 'cat', 'room', 'tic']\n",
      "\n",
      "\n",
      "The top 15 words for topic 11\n",
      "['love', 'reveal', 'bumper', 'not', 'idea', 'friend', 'find', 'group', 'gift', 'baby shower', 'gender', 'girl', 'boy', 'baby', 'shower']\n",
      "\n",
      "\n",
      "The top 15 words for topic 12\n",
      "['wean', 'sleep night', 'bed', 'baby', 'eat', 'not', 'nursing', 'nap', 'old', 'hour', 'feed', 'wake', 'nurse', 'night', 'sleep']\n",
      "\n",
      "\n",
      "The top 15 words for topic 13\n",
      "['pregnant', 'live', 'law', 'mil', 'ask', 'come', 'people', 'mom', 'parent', 'friend', 'husband', 'not', 'baby', 'tell', 'family']\n",
      "\n",
      "\n",
      "The top 15 words for topic 14\n",
      "['lc', 'green', 'breast feeding', 'breast', 'feeding', 'pediatrician', 'latch', 'lactation consultant', 'consultant', 'lactation', 'lip', 'poop', 'tongue tie', 'tongue', 'tie']\n",
      "\n",
      "\n",
      "The top 15 words for topic 15\n",
      "['fat', 'baby', 'pregnant woman', 'tell', 'big', 'ask', 'pregnancy', 'not', 'comment', 'woman', 'look', 'people', 'belly', 'bump', 'pregnant']\n",
      "\n",
      "\n",
      "The top 15 words for topic 16\n",
      "['boob', 'left', 'milk', 'massage', 'lump', 'bite', 'clogged duct', 'clogged', 'tooth', 'clog', 'pain', 'mastitis', 'breast', 'duct', 'nipple']\n",
      "\n",
      "\n",
      "The top 15 words for topic 17\n",
      "['price', 'th percentile', 'worth', 'buy', 'percentile', 'system', 'review', 'infant', 'baby', 'bassinet', 'travel', 'car seat', 'car', 'stroller', 'seat']\n",
      "\n",
      "\n",
      "The top 15 words for topic 18\n",
      "['panic', 'attack', 'cry', 'hormone', 'bad', 'ppd', 'mental', 'help', 'sex', 'pregnant', 'baby', 'pregnancy', 'not', 'depression', 'anxiety']\n",
      "\n",
      "\n",
      "The top 15 words for topic 19\n",
      "['try', 'pumping', 'supplement', 'not', 'baby', 'breastfeed', 'work', 'oz', 'formula', 'bottle', 'feed', 'breast', 'milk', 'supply', 'pump']\n",
      "\n",
      "\n",
      "The top 15 words for topic 20\n",
      "['burp', 'spit', 'try', 'baby', 'fall', 'feeding', 'fall asleep', 'eat', 'minute', 'latch', 'boob', 'asleep', 'breast', 'old', 'feed']\n",
      "\n",
      "\n",
      "The top 15 words for topic 21\n",
      "['stop', 'not', 'mouth', 'feed', 'try', 'nursing', 'nipple shield', 'boob', 'old', 'nurse', 'breastfeed', 'shield', 'wean', 'latch', 'nipple']\n",
      "\n",
      "\n",
      "The top 15 words for topic 22\n",
      "['oz', 'drink', 'cup', 'breast', 'frozen', 'fridge', 'stash', 'freeze', 'breast milk', 'breastmilk', 'bag', 'pump', 'freezer', 'bottle', 'milk']\n",
      "\n",
      "\n",
      "The top 15 words for topic 23\n",
      "['surgery', 'plan', 'not', 'ob', 'breech', 'experience', 'schedule', 'deliver', 'doctor', 'labor', 'delivery', 'hospital', 'baby', 'birth', 'section']\n",
      "\n",
      "\n",
      "The top 15 words for topic 24\n",
      "['anatomy', 'experience', 'today', 'heartbeat', 'tell', 'result', 'pregnancy', 'not', 'test', 'blood', 'baby', 'appointment', 'doctor', 'scan', 'ultrasound']\n",
      "\n",
      "\n",
      "The top 15 words for topic 25\n",
      "['baby', 'cover', 'need', 'boss', 'company', 'pregnant', 'home', 'not', 'maternity leave', 'maternity', 'pay', 'insurance', 'leave', 'job', 'work']\n",
      "\n",
      "\n",
      "The top 15 words for topic 26\n",
      "['cup', 'fit', 'shirt', 'leak', 'recommendation', 'pad', 'maternity', 'pregnancy', 'boob', 'nursing bra', 'pillow', 'size', 'wear', 'nursing', 'bra']\n",
      "\n",
      "\n",
      "The top 15 words for topic 27\n",
      "['drink', 'old son', 'idea', 'son', 'pacifier', 'new baby', 'new', 'old', 'mommy', 'ring', 'glass', 'carrier', 'baby', 'sale', 'wedding']\n",
      "\n",
      "\n",
      "The top 15 words for topic 28\n",
      "['mother', 'feedback', 'public', 'baby arrive', 'arrive', 'stop breastfeed', 'share experience', 'view', 'experience', 'study', 'link', 'research', 'share', 'breastfeeding', 'breastfeed']\n",
      "\n",
      "\n",
      "The top 15 words for topic 29\n",
      "['right', 'sit', 'help', 'not', 'walk', 'muscle', 'pregnancy', 'rib', 'bad', 'foot', 'pelvic', 'hurt', 'leg', 'hip', 'pain']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3170.1336350744855"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_lda(pospar, 'lemma_tokens', n_topics = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30616, 6724)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_tvec = TfidfVectorizer(max_df = 0.98, min_df = 0.001, ngram_range = (1,2), stop_words = full_stop_words)\n",
    "X = lsa_tvec.fit_transform(health['lemma_tokens'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=10, random_state=42)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model = TruncatedSVD(n_components = 10, random_state = 42)\n",
    "svd_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0: \n",
      "period\n",
      "not\n",
      "start\n",
      "pain\n",
      "pill\n",
      "sex\n",
      "control\n",
      "help\n",
      "birth\n",
      "birth control\n",
      "come\n",
      "try\n",
      "doctor\n",
      "look\n",
      "normal\n",
      "\n",
      "Topic 1: \n",
      "period\n",
      "pill\n",
      "birth control\n",
      "birth\n",
      "control\n",
      "bleed\n",
      "cramp\n",
      "pregnancy\n",
      "pregnant\n",
      "cycle\n",
      "iud\n",
      "blood\n",
      "test\n",
      "normal\n",
      "spot\n",
      "\n",
      "Topic 2: \n",
      "infection\n",
      "yeast\n",
      "yeast infection\n",
      "discharge\n",
      "uti\n",
      "vagina\n",
      "bv\n",
      "smell\n",
      "pain\n",
      "sex\n",
      "antibiotic\n",
      "symptom\n",
      "doctor\n",
      "vaginal\n",
      "test\n",
      "\n",
      "Topic 3: \n",
      "hair\n",
      "shave\n",
      "yeast\n",
      "infection\n",
      "yeast infection\n",
      "wax\n",
      "wash\n",
      "period\n",
      "grow\n",
      "dye\n",
      "birth control\n",
      "birth\n",
      "pill\n",
      "control\n",
      "shampoo\n",
      "\n",
      "Topic 4: \n",
      "birth control\n",
      "control\n",
      "birth\n",
      "pill\n",
      "yeast\n",
      "yeast infection\n",
      "infection\n",
      "control pill\n",
      "friend\n",
      "pack\n",
      "effect\n",
      "work\n",
      "good\n",
      "new\n",
      "start birth\n",
      "\n",
      "Topic 5: \n",
      "pain\n",
      "cyst\n",
      "iud\n",
      "birth\n",
      "birth control\n",
      "control\n",
      "doctor\n",
      "breast\n",
      "ovarian\n",
      "hurt\n",
      "ovarian cyst\n",
      "ovary\n",
      "painful\n",
      "ultrasound\n",
      "experience\n",
      "\n",
      "Topic 6: \n",
      "wear\n",
      "bra\n",
      "dress\n",
      "look\n",
      "breast\n",
      "size\n",
      "clothe\n",
      "smell\n",
      "buy\n",
      "boob\n",
      "makeup\n",
      "jean\n",
      "fit\n",
      "skin\n",
      "cup\n",
      "\n",
      "Topic 7: \n",
      "iud\n",
      "period\n",
      "friend\n",
      "pain\n",
      "yeast\n",
      "yeast infection\n",
      "infection\n",
      "cramp\n",
      "cyst\n",
      "work\n",
      "copper\n",
      "bad\n",
      "heavy\n",
      "copper iud\n",
      "experience\n",
      "\n",
      "Topic 8: \n",
      "iud\n",
      "sex\n",
      "bleed\n",
      "smell\n",
      "copper\n",
      "copper iud\n",
      "mirena\n",
      "vagina\n",
      "discharge\n",
      "tampon\n",
      "string\n",
      "insertion\n",
      "insert\n",
      "pill\n",
      "use\n",
      "\n",
      "Topic 9: \n",
      "test\n",
      "iud\n",
      "pregnancy\n",
      "negative\n",
      "pregnant\n",
      "pregnancy test\n",
      "pap\n",
      "late\n",
      "result\n",
      "doctor\n",
      "woman\n",
      "smear\n",
      "symptom\n",
      "copper\n",
      "come\n"
     ]
    }
   ],
   "source": [
    "terms = lsa_tvec.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:15]\n",
    "    print()\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BabyBumps               33280\n",
       "breastfeeding           15593\n",
       "postpartumdepression      221\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pospar['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245.47"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pospar) * 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
